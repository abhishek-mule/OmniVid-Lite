from pathlib import Path
from fastapi import APIRouter, HTTPException, status, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Dict
import asyncio

from app.core.config import settings
from app.services.security import require_user
from app.services.job_store import (
    create_job,
    get_job,
    update_job_status,
    update_job_progress,
    list_jobs,
    cancel_job,
    cleanup_old_jobs,
    JobStatus,
    Job
)
from app.services.video_renderer import create_enhanced_text_video
from app.services.llm_client import llm_client
from app.services.templates import (
    list_templates,
    get_template,
    get_template_categories,
    apply_template_variables,
    suggest_templates_for_prompt,
    VideoTemplate
)
from app.services.errors import QuotaError
from app.services.logging_service import audit_logger
import uuid
import logging

router = APIRouter()


# ---------------------- MODELS ----------------------

class RenderRequest(BaseModel):
    prompt: str
    creative: Optional[bool] = False
    duration: Optional[int] = 5
    resolution: Optional[str] = "1080p"
    style: Optional[str] = "minimal"
    animation: Optional[str] = "fade"


class JobStatusResponse(BaseModel):
    job_id: str
    status: str
    progress: float
    output_path: Optional[str]
    error: Optional[str]
    created_at: str
    updated_at: str


class JobSummary(BaseModel):
    job_id: str
    status: str
    output_path: Optional[str]
    created_at: str
    updated_at: str


class JobListResponse(BaseModel):
    jobs: List[JobSummary]


# ---------------------- ROUTES ----------------------

@router.post("/render", status_code=202)
async def start_render(req: RenderRequest, user=Depends(require_user), request: Request = None):
    """Create + start a video render request."""

    prompt = req.prompt.strip()
    if not prompt:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "Prompt must be non-empty")

    # Generate unique job ID
    job_id = str(uuid.uuid4())

    try:
        # Create job in store
        job = await create_job(job_id, req.prompt, user.id, req.creative)
        audit_logger.log_job_event(job.id, "job_created", {"prompt": prompt, "creative": req.creative}, user.id)

        # Start video rendering in background with enhanced parameters
        asyncio.create_task(process_video_render(job_id, req))

        base = str(request.base_url).rstrip("/")

        return {
            "job_id": job.id,
            "status": job.status.value,
            "poll_url": f"{base}/api/v1/render/status/{job.id}",
            "download_url": f"{base}/api/v1/render/download/{job.id}",
        }

    except Exception as e:
        audit_logger.log_job_event("unknown", "job_creation_failed", {"error": str(e)}, user.id)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, f"Failed to create job: {str(e)}")


@router.get("/status/{job_id}", response_model=JobStatusResponse)
async def get_status(job_id: str, user=Depends(require_user)):
    """Return job status."""

    job = await get_job(job_id)
    if not job:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Job not found")

    if job.user_id != user.id:
        raise HTTPException(status.HTTP_403_FORBIDDEN, "Unauthorized")

    return JobStatusResponse(
        job_id=job.id,
        status=job.status.value,
        progress=job.progress,
        output_path=job.output_path,
        error=job.error,
        created_at=job.created_at.isoformat(),
        updated_at=job.updated_at.isoformat(),
    )


@router.patch("/cancel/{job_id}")
async def cancel_job_endpoint(job_id: str, user=Depends(require_user)):
    """Cancel a job."""

    job = await get_job(job_id)
    if not job:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Job not found")

    if job.user_id != user.id:
        raise HTTPException(status.HTTP_403_FORBIDDEN, "Unauthorized")

    if not await cancel_job(job_id):
        raise HTTPException(status.HTTP_409_CONFLICT, "Cannot cancel at this stage")

    audit_logger.log_job_event(job_id, "job_cancelled", {}, user.id)
    return {"message": "Cancellation requested"}


@router.get("/download/{job_id}")
async def download(job_id: str, user=Depends(require_user)):
    """Serve rendered file."""

    job = await get_job(job_id)
    if not job:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Job not found")

    if job.user_id != user.id:
        raise HTTPException(status.HTTP_403_FORBIDDEN, "Unauthorized")

    if job.status != JobStatus.COMPLETED:
        raise HTTPException(status.HTTP_425_TOO_EARLY, f"Job not ready: {job.status.value}")

    if not job.output_path:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Output missing")

    path = Path(job.output_path)
    if not path.exists():
        raise HTTPException(status.HTTP_410_GONE, "File expired")

    def stream(chunk=8192):
        with open(path, "rb") as f:
            while data := f.read(chunk):
                yield data

    audit_logger.log_job_event(job_id, "file_downloaded", {"filename": path.name}, user.id)

    return StreamingResponse(
        stream(),
        media_type="video/mp4",
        headers={"Content-Disposition": f"attachment; filename={path.name}"}
    )


@router.post("/cleanup")
async def cleanup_system(days_old: int = 7, user=Depends(require_user)):
    """Clean up old jobs and orphaned files (admin operation)"""

    # Basic auth check - in real app, check for admin role
    if user.id != "demo_user":  # Placeholder for admin check
        raise HTTPException(status.HTTP_403_FORBIDDEN, "Admin access required")

    old_jobs_cleaned = await cleanup_old_jobs(days_old)
    # Note: orphaned file cleanup not implemented in simple version

    return {
        "message": f"Cleanup completed: {old_jobs_cleaned} old jobs removed",
        "old_jobs_removed": old_jobs_cleaned,
        "orphaned_files_removed": 0
    }


@router.get("/ai-status")
async def get_ai_status():
    """Check the status of AI services (local LLM and fallback availability)"""
    from app.services.llm_client import llm_client

    ai_status = {
        "local_llm": {
            "enabled": llm_client.use_local,
            "model": llm_client.ollama_model if llm_client.use_local else None,
            "url": llm_client.ollama_url if llm_client.use_local else None,
            "status": "unknown"
        },
        "openai": {
            "enabled": bool(llm_client.openai_key),
            "model": llm_client.openai_model,
            "status": "unknown"
        },
        "overall_status": "checking"
    }

    # Test local LLM if enabled
    if llm_client.use_local:
        try:
            # Quick test with a simple prompt
            test_result = await asyncio.wait_for(
                llm_client.generate_dsl("test", temperature=0.1, max_tokens=50),
                timeout=5.0
            )
            ai_status["local_llm"]["status"] = "available" if test_result else "error"
        except Exception as e:
            ai_status["local_llm"]["status"] = "unavailable"
            ai_status["local_llm"]["error"] = str(e)

    # Test OpenAI if key is available
    if llm_client.openai_key:
        try:
            # Quick test
            test_result = await asyncio.wait_for(
                llm_client.generate_dsl("test", temperature=0.1, max_tokens=50),
                timeout=5.0
            )
            ai_status["openai"]["status"] = "available" if test_result else "error"
        except Exception as e:
            ai_status["openai"]["status"] = "unavailable"
            ai_status["openai"]["error"] = str(e)

    # Determine overall status
    if ai_status["local_llm"]["status"] == "available":
        ai_status["overall_status"] = "ai_available"
        ai_status["message"] = "AI-powered generation available (Local LLM)"
    elif ai_status["openai"]["status"] == "available":
        ai_status["overall_status"] = "ai_available"
        ai_status["message"] = "AI-powered generation available (OpenAI)"
    else:
        ai_status["overall_status"] = "fallback_only"
        ai_status["message"] = "Using intelligent text enhancement (AI offline)"

    return ai_status


@router.get("/templates")
async def get_templates(category: str = None):
    """Get list of available video templates"""
    templates = list_templates(category)
    return {"templates": [t.dict() for t in templates]}


@router.get("/templates/categories")
async def get_template_categories_endpoint():
    """Get list of template categories"""
    categories = get_template_categories()
    return {"categories": categories}


@router.get("/templates/{template_id}")
async def get_template_endpoint(template_id: str):
    """Get a specific template by ID"""
    try:
        template = get_template(template_id)
        return template.dict()
    except ValueError as e:
        raise HTTPException(status.HTTP_404_NOT_FOUND, str(e))


@router.post("/templates/{template_id}/apply")
async def apply_template(template_id: str, variables: Dict[str, str]):
    """Apply variables to a template and return customized version"""
    try:
        template = get_template(template_id)
        customized = apply_template_variables(template, variables)
        return customized.dict()
    except ValueError as e:
        raise HTTPException(status.HTTP_404_NOT_FOUND, str(e))


class TemplateSuggestionRequest(BaseModel):
    prompt: str

@router.post("/templates/suggest")
async def suggest_templates(request: TemplateSuggestionRequest):
    """Suggest templates based on prompt analysis"""
    suggestions = suggest_templates_for_prompt(request.prompt)
    return {"suggestions": suggestions}


# ---------------------- HELPER FUNCTIONS ----------------------

def _categorize_ai_error(error: Exception) -> str:
    """
    Categorize AI errors for better error handling and user messaging.
    Returns: 'temporary', 'permanent', or 'unknown'
    """
    error_str = str(error).lower()

    # Permanent errors (no point retrying)
    permanent_indicators = [
        "authentication failed", "invalid api key", "unauthorized",
        "model not found", "model does not exist", "forbidden",
        "quota exceeded", "billing", "payment required"
    ]

    # Temporary errors (might work if retried)
    temporary_indicators = [
        "timeout", "connection", "network", "server error",
        "rate limit", "too many requests", "service unavailable",
        "internal server error", "bad gateway", "gateway timeout"
    ]

    for indicator in permanent_indicators:
        if indicator in error_str:
            return "permanent"

    for indicator in temporary_indicators:
        if indicator in error_str:
            return "temporary"

    return "unknown"

async def enhance_prompt_for_video(prompt: str) -> str:
    """
    Enhance a simple prompt into more engaging, video-friendly text.
    This acts as a fallback when AI generation is not available.
    """
    prompt_lower = prompt.lower().strip()

    # Dictionary of enhancements for common prompts
    enhancements = {
        # Action/Motion prompts
        "a ball is dancing": "ðŸŽ¾ BALL DANCING! âš¡ Bounce â€¢ Groove â€¢ Spin â€¢ ENERGY! ðŸŽ¾",
        "ball is dancing": "ðŸŽ¾ BALL DANCING! âš¡ Bounce â€¢ Groove â€¢ Spin â€¢ ENERGY! ðŸŽ¾",
        "i ball is dancing": "ðŸŽ¾ BALL DANCING! âš¡ Bounce â€¢ Groove â€¢ Spin â€¢ ENERGY! ðŸŽ¾",
        "ball dancing": "ðŸŽ¾ BALL DANCING! âš¡ Bounce â€¢ Groove â€¢ Spin â€¢ ENERGY! ðŸŽ¾",

        # Technology prompts
        "hello world": "ðŸš€ HELLO WORLD! âš¡ Code â€¢ Create â€¢ Innovate â€¢ FUTURE! ðŸš€",
        "hello": "ðŸŒŸ HELLO! âš¡ Welcome â€¢ Connect â€¢ Discover â€¢ MAGIC! ðŸŒŸ",

        # Creative prompts
        "create something": "ðŸŽ¨ CREATING MAGIC! âš¡ Design â€¢ Build â€¢ Innovate â€¢ ART! ðŸŽ¨",
        "make art": "ðŸŽ¨ CREATING ART! âš¡ Colors â€¢ Shapes â€¢ Beauty â€¢ EXPRESSION! ðŸŽ¨",

        # Learning prompts
        "learn coding": "ðŸ’» LEARN TO CODE! âš¡ Logic â€¢ Create â€¢ Build â€¢ FUTURE! ðŸ’»",
        "study": "ðŸ“š STUDY TIME! âš¡ Learn â€¢ Grow â€¢ Achieve â€¢ SUCCESS! ðŸ“š",

        # Business prompts
        "start business": "ðŸš€ START YOUR BUSINESS! âš¡ Vision â€¢ Action â€¢ Growth â€¢ SUCCESS! ðŸš€",
        "entrepreneur": "ðŸ’¼ ENTREPRENEUR LIFE! âš¡ Ideas â€¢ Action â€¢ Scale â€¢ WIN! ðŸ’¼",

        # Motivation prompts
        "never give up": "ðŸ’ª NEVER GIVE UP! âš¡ Persist â€¢ Fight â€¢ Win â€¢ CHAMPION! ðŸ’ª",
        "motivation": "ðŸ”¥ GET MOTIVATED! âš¡ Action â€¢ Drive â€¢ Success â€¢ ACHIEVE! ðŸ”¥",

        # Generic fallbacks
        "test": "ðŸ§ª TESTING IN PROGRESS! âš¡ Experiment â€¢ Learn â€¢ Improve â€¢ EVOLVE! ðŸ§ª",
        "demo": "ðŸŽ¬ DEMO TIME! âš¡ Showcase â€¢ Present â€¢ Impress â€¢ WOW! ðŸŽ¬",
    }

    # Check for exact matches first
    if prompt_lower in enhancements:
        return enhancements[prompt_lower]

    # Check for partial matches
    for key, value in enhancements.items():
        if key in prompt_lower:
            return value

    # Generic enhancement based on keywords
    words = prompt_lower.split()

    # Add energy indicators
    energy_words = ["fast", "quick", "speed", "power", "energy", "dynamic", "action"]
    creative_words = ["create", "make", "build", "design", "art", "draw"]
    learning_words = ["learn", "study", "teach", "understand", "know"]
    business_words = ["business", "money", "success", "work", "job"]

    if any(word in prompt_lower for word in energy_words):
        enhanced = f"âš¡ {prompt.upper()}! âš¡ ENERGY â€¢ POWER â€¢ SPEED â€¢ ACTION! âš¡"
    elif any(word in prompt_lower for word in creative_words):
        enhanced = f"ðŸŽ¨ {prompt.upper()}! ðŸŽ¨ CREATE â€¢ DESIGN â€¢ BUILD â€¢ ART! ðŸŽ¨"
    elif any(word in prompt_lower for word in learning_words):
        enhanced = f"ðŸ“š {prompt.upper()}! ðŸ“š LEARN â€¢ GROW â€¢ ACHIEVE â€¢ SUCCESS! ðŸ“š"
    elif any(word in prompt_lower for word in business_words):
        enhanced = f"ðŸš€ {prompt.upper()}! ðŸš€ VISION â€¢ ACTION â€¢ GROWTH â€¢ SUCCESS! ðŸš€"
    else:
        # Default enhancement with emojis and energy
        enhanced = f"âœ¨ {prompt.upper()}! âš¡ EXCITING â€¢ AMAZING â€¢ WONDERFUL â€¢ MAGIC! âœ¨"

    return enhanced

async def process_video_render(job_id: str, req: RenderRequest):
    """Process the video rendering in the background with enhanced parameters."""
    import os
    from pathlib import Path

    logger = logging.getLogger(__name__)

    try:
        # Log job started with parameters
        params = {
            "prompt": req.prompt,
            "duration": req.duration,
            "resolution": req.resolution,
            "style": req.style,
            "animation": req.animation,
            "creative": req.creative
        }
        audit_logger.log_job_event(job_id, "job_started", params, "system")
        logger.info(f"Job {job_id} started processing with enhanced parameters: {params}")

        # Update job status to processing
        await update_job_status(job_id, JobStatus.PROCESSING)
        audit_logger.log_job_event(job_id, "status_changed", {"status": "processing"}, "system")
        logger.info(f"Job {job_id} status changed to PROCESSING")

        # Create storage directory if it doesn't exist
        storage_dir = Path("storage")
        storage_dir.mkdir(exist_ok=True)

        job_dir = storage_dir / "jobs" / job_id
        job_dir.mkdir(parents=True, exist_ok=True)

        output_path = job_dir / "output.mp4"
        logger.info(f"Job {job_id} storage directory created: {job_dir}")

        # Progress callback
        def progress_callback(progress: int):
            asyncio.create_task(update_job_progress(job_id, progress))
            logger.debug(f"ðŸ“Š Job {job_id} progress: {progress}%")

        # Step 1: Generate scene DSL using AI
        await update_job_progress(job_id, 5)
        audit_logger.log_job_event(job_id, "ai_generation_started", params, "system")
        logger.info(f"Job {job_id} starting AI scene generation for prompt: '{req.prompt}'")

        scene_dsl = None
        ai_error_details = None

        try:
            scene_dsl = await llm_client.generate_dsl(req.prompt)
            logger.info(f"Job {job_id} AI scene generation completed")
        except Exception as e:
            ai_error_details = str(e)
            error_type = _categorize_ai_error(e)

            if error_type == "temporary":
                logger.warning(f"Job {job_id} Temporary AI error ({e}), falling back to enhanced text rendering")
            elif error_type == "permanent":
                logger.error(f"Job {job_id} Permanent AI error ({e}), falling back to enhanced text rendering")
            else:
                logger.warning(f"Job {job_id} Unknown AI error ({e}), falling back to enhanced text rendering")

            # Log the error details for debugging
            audit_logger.log_job_event(job_id, "ai_generation_failed", {
                "error": ai_error_details,
                "error_type": error_type,
                "fallback_used": True
            }, "system")

        # If AI generation failed, enhance the prompt for better text rendering
        if scene_dsl is None:
            req.prompt = await enhance_prompt_for_video(req.prompt)
            logger.info(f"Job {job_id} enhanced prompt for fallback: '{req.prompt}'")

        # Step 2: Create the enhanced video with AI-generated content or fallback
        await update_job_progress(job_id, 10)
        audit_logger.log_job_event(job_id, "render_started", {
            "output_path": str(output_path),
            "ai_generated": scene_dsl is not None,
            **params
        }, "system")
        logger.info(f"Job {job_id} starting enhanced video render to: {output_path}")

        if scene_dsl:
            # Use AI-generated scene DSL for advanced rendering
            created_path = create_scene_based_video(
                scene_dsl,
                str(output_path),
                progress_callback,
                duration=req.duration,
                resolution=req.resolution,
                style=req.style,
                animation=req.animation
            )
        else:
            # Fallback to enhanced text video
            created_path = create_enhanced_text_video(
                req.prompt,
                str(output_path),
                progress_callback,
                duration=req.duration,
                resolution=req.resolution,
                style=req.style,
                animation=req.animation
            )

        # Verify file was created
        if not os.path.exists(created_path):
            raise Exception("Video file was not created")

        # Update with final path
        await update_job_progress(job_id, 100, created_path)
        await update_job_status(job_id, JobStatus.COMPLETED)

        audit_logger.log_job_event(job_id, "job_completed", {
            "output_path": created_path,
            **params
        }, "system")
        logger.info(f"Job {job_id} COMPLETED successfully with enhanced video, output: {created_path}")

    except Exception as e:
        error_msg = f"Enhanced video rendering failed: {str(e)}"
        logger.error(f"Job {job_id} FAILED: {error_msg}", exc_info=True)
        audit_logger.log_job_event(job_id, "job_failed", {"error": error_msg}, "system")
        await update_job_status(job_id, JobStatus.FAILED, error_msg)


# Update the list_jobs endpoint to use the new job store
@router.get("/jobs", response_model=JobListResponse)
async def list_user_jobs(limit: int = 50, user=Depends(require_user)):
    """List jobs for the authenticated user."""

    user_jobs = await list_jobs(user.id, limit)

    return JobListResponse(
        jobs=[
            JobSummary(
                job_id=job.id,
                status=job.status.value,
                output_path=job.output_path,
                created_at=job.created_at.isoformat(),
                updated_at=job.updated_at.isoformat(),
            )
            for job in user_jobs
        ]
    )
